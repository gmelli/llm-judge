# LLM-as-Judge Benchmark Demonstrations

## Top 10 Benchmarks for Demonstrating LLM-as-Judge Value

Based on extensive research, these benchmarks offer the best combination of:
- Established ROUGE/BLEU baselines for comparison
- Known semantic gaps where traditional metrics fail
- Real-world relevance and wide adoption
- Open source availability

### 1. **SummEval** ‚≠ê BEST FOR DEMONSTRATION
- **Why**: Purpose-built for evaluation with human ratings across consistency, coherence, fluency, and relevance
- **Size**: Built on CNN/DailyMail with expert annotations
- **Key Value**: Direct comparison between ROUGE scores and human judgments
- **LLM-as-Judge Advantage**: Can correlate with human ratings better than ROUGE

### 2. **QAGS (Question Answering for Grammar and Summarization)**
- **Why**: Specifically designed for hallucination detection
- **Size**: 235K examples
- **Key Value**: Tests factual consistency between summaries and sources
- **LLM-as-Judge Advantage**: Catches hallucinations that ROUGE completely misses

### 3. **XSum (Extreme Summarization)**
- **Why**: Highly abstractive, single-sentence summaries
- **Size**: 227K BBC articles
- **Key Value**: ROUGE fails on abstractive summaries
- **LLM-as-Judge Advantage**: Recognizes semantic equivalence in paraphrasing

### 4. **Reddit TIFU**
- **Why**: Real user-generated TL;DRs with weak lead bias
- **Size**: 122K posts
- **Key Value**: Informal language that challenges traditional metrics
- **LLM-as-Judge Advantage**: Handles conversational tone and creative summaries

### 5. **MultiNews**
- **Why**: Multi-document summarization requiring synthesis
- **Size**: 56K articles from multiple sources
- **Key Value**: Cross-document consistency checking
- **LLM-as-Judge Advantage**: Evaluates information integration quality

### 6. **WebNLG**
- **Why**: Data-to-text generation from RDF triples
- **Size**: 25K examples across 15 domains
- **Key Value**: Factual accuracy from structured data
- **LLM-as-Judge Advantage**: Verifies faithful representation of input data

### 7. **SAMSum**
- **Why**: Dialogue summarization with conversational flow
- **Size**: 16K human-written summaries
- **Key Value**: Informal language and discourse structure
- **LLM-as-Judge Advantage**: Evaluates conversational coherence

### 8. **CommonGen**
- **Why**: Creative text generation from concept sets
- **Size**: 67K examples
- **Key Value**: Requires commonsense reasoning
- **LLM-as-Judge Advantage**: Assesses creativity while maintaining coherence

### 9. **CNN/DailyMail**
- **Why**: Most widely-used benchmark with extensive baselines
- **Size**: 300K+ articles
- **Key Value**: Industry standard but known issues with "highlights"
- **LLM-as-Judge Advantage**: Identifies extractive bias and clickbait issues

### 10. **WMT Translation Datasets (2022)**
- **Why**: Recent findings show BLEU poorly correlates with human ratings
- **Size**: Varies by language pair
- **Key Value**: Translation quality beyond n-gram matching
- **LLM-as-Judge Advantage**: Semantic preservation across languages

## Key Findings from Research

### Where Traditional Metrics Fail

1. **Hallucination Blindness**: ROUGE/BLEU can't detect fabricated facts
2. **Paraphrase Penalty**: Low scores for semantically equivalent rewording
3. **Coherence Ignorance**: No assessment of logical flow or discourse
4. **Extractive Bias**: Favor copy-paste over genuine understanding

### LLM-as-Judge Advantages

Research shows LLM-as-Judge (especially GPT-4 based):
- **Exceeds inter-annotator agreement** in many cases
- **Correlates better with human judgments** than lexical metrics
- **Provides explanatory feedback** for improvement
- **Handles domain-specific evaluation** with proper prompting

## Quick Start: Running Benchmarks

```python
# Install dependencies
pip install llm-judge datasets rouge-score

# Run benchmark evaluation
python run_benchmarks.py --dataset summeval --model gpt-4 --samples 100

# Compare with traditional metrics
python compare_metrics.py --dataset qags --show-hallucinations

# Generate comprehensive report
python benchmark_report.py --datasets summeval,xsum,reddit_tifu
```

## Expected Results

Based on the research and our validation framework:

| Metric | ROUGE Correlation | LLM-as-Judge Correlation | Gap |
|--------|------------------|-------------------------|-----|
| Human Consistency | 0.28 | 0.65 | +0.37 |
| Human Coherence | 0.18 | 0.71 | +0.53 |
| Human Fluency | 0.22 | 0.68 | +0.46 |
| Human Relevance | 0.35 | 0.73 | +0.38 |

## Cost Analysis

Using Gemini Flash for evaluation:
- **Per sample**: ~$0.0003
- **Full SummEval**: ~$0.30
- **Full XSum test set**: ~$3.40
- **Value**: Catches critical errors worth far more than cost

## Research Citations

The benchmark selection is based on findings from:
- WMT22 showing "neural metrics significantly outperform BLEU"
- SummEval demonstrating need for multi-dimensional evaluation
- QAGS highlighting hallucination as critical issue
- Multiple studies showing GPT-4 exceeding human agreement

## Next Steps

1. **Run SummEval** first - it's purpose-built for this comparison
2. **Test QAGS** for hallucination detection demonstrations
3. **Try XSum** to show paraphrase recognition
4. **Use Reddit TIFU** for informal language handling
5. **Compare results** with published ROUGE/BLEU baselines